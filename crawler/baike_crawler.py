# ç™¾åº¦ç™¾ç§‘æ¤ç‰©çˆ¬è™«
# çˆ¬å–å¸¸è§æ¤ç‰©çš„ä¸­æ–‡æ•°æ®

import requests
from bs4 import BeautifulSoup
import json
import time
import re

class BaikePlantCrawler:
    def __init__(self):
        self.base_url = "https://baike.baidu.com/item/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.plants_data = []
    
    def get_plant_info(self, plant_name):
        """è·å–å•ä¸ªæ¤ç‰©ä¿¡æ¯"""
        try:
            url = f"{self.base_url}{plant_name}"
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                print(f"âŒ è·å– {plant_name} å¤±è´¥: {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ•°æ®
            plant_data = {
                'chinese_name': plant_name,
                'scientific_name': self.extract_scientific_name(soup),
                'english_name': self.extract_english_name(soup),
                'family': self.extract_family(soup),
                'genus': self.extract_genus(soup),
                'description': self.extract_description(soup),
                'alias': self.extract_alias(soup),
                'morphology': self.extract_morphology(soup),
                'habitat': self.extract_habitat(soup),
                'distribution': self.extract_distribution(soup),
                'flowering_period': self.extract_flowering_period(soup),
                'image_url': self.extract_image(soup)
            }
            
            print(f"âœ… æˆåŠŸè·å–: {plant_name}")
            return plant_data
            
        except Exception as e:
            print(f"âŒ çˆ¬å– {plant_name} å‡ºé”™: {str(e)}")
            return None
    
    def extract_scientific_name(self, soup):
        """æå–å­¦å"""
        try:
            # æŸ¥æ‰¾åŒ…å«"å­¦å"çš„æ ‡ç­¾
            label = soup.find('dt', string=re.compile('å­¦å'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    return value.get_text(strip=True)
        except:
            pass
        return None
    
    def extract_english_name(self, soup):
        """æå–è‹±æ–‡å"""
        try:
            label = soup.find('dt', string=re.compile('è‹±æ–‡å'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    return value.get_text(strip=True)
        except:
            pass
        return None
    
    def extract_family(self, soup):
        """æå–ç§‘"""
        try:
            label = soup.find('dt', string=re.compile('ç§‘'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    return value.get_text(strip=True)
        except:
            pass
        return None
    
    def extract_genus(self, soup):
        """æå–å±"""
        try:
            label = soup.find('dt', string=re.compile('å±'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    return value.get_text(strip=True)
        except:
            pass
        return None
    
    def extract_description(self, soup):
        """æå–ç®€ä»‹"""
        try:
            # è·å–ç¬¬ä¸€æ®µæ‘˜è¦
            summary = soup.find('div', class_='lemma-summary')
            if summary:
                paragraphs = summary.find_all('div', class_='para')
                if paragraphs:
                    return paragraphs[0].get_text(strip=True)[:500]
        except:
            pass
        return None
    
    def extract_alias(self, soup):
        """æå–åˆ«å"""
        try:
            label = soup.find('dt', string=re.compile('åˆ«.*å'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    aliases = value.get_text(strip=True)
                    return json.dumps(aliases.split('ã€'), ensure_ascii=False)
        except:
            pass
        return None
    
    def extract_morphology(self, soup):
        """æå–å½¢æ€ç‰¹å¾"""
        try:
            # æŸ¥æ‰¾"å½¢æ€ç‰¹å¾"æ ‡é¢˜
            heading = soup.find(['h2', 'h3'], string=re.compile('å½¢æ€ç‰¹å¾'))
            if heading:
                content = heading.find_next('div', class_='para')
                if content:
                    return content.get_text(strip=True)[:500]
        except:
            pass
        return None
    
    def extract_habitat(self, soup):
        """æå–ç”Ÿé•¿ç¯å¢ƒ"""
        try:
            heading = soup.find(['h2', 'h3'], string=re.compile('ç”Ÿé•¿ç¯å¢ƒ|ç”Ÿå¢ƒ'))
            if heading:
                content = heading.find_next('div', class_='para')
                if content:
                    return content.get_text(strip=True)[:300]
        except:
            pass
        return None
    
    def extract_distribution(self, soup):
        """æå–åˆ†å¸ƒåœ°åŒº"""
        try:
            heading = soup.find(['h2', 'h3'], string=re.compile('åˆ†å¸ƒ.*èŒƒå›´|äº§åœ°'))
            if heading:
                content = heading.find_next('div', class_='para')
                if content:
                    return content.get_text(strip=True)[:300]
        except:
            pass
        return None
    
    def extract_flowering_period(self, soup):
        """æå–èŠ±æœŸ"""
        try:
            label = soup.find('dt', string=re.compile('èŠ±æœŸ'))
            if label:
                value = label.find_next_sibling('dd')
                if value:
                    return value.get_text(strip=True)
        except:
            pass
        return None
    
    def extract_image(self, soup):
        """æå–å›¾ç‰‡"""
        try:
            # è·å–ä¸»å›¾
            img = soup.find('div', class_='summary-pic').find('img')
            if img and img.get('src'):
                return 'https:' + img['src']
        except:
            pass
        return None
    
    def crawl_plants(self, plant_list):
        """æ‰¹é‡çˆ¬å–æ¤ç‰©"""
        for i, plant_name in enumerate(plant_list, 1):
            print(f"\n[{i}/{len(plant_list)}] æ­£åœ¨çˆ¬å–: {plant_name}")
            
            plant_data = self.get_plant_info(plant_name)
            if plant_data:
                self.plants_data.append(plant_data)
            
            # å»¶æ—¶é¿å…è¢«å°
            time.sleep(2)
        
        print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±è·å– {len(self.plants_data)} æ¡æ•°æ®")
    
    def save_to_json(self, filename='plants_data.json'):
        """ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.plants_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

# å¸¸è§æ¤ç‰©åˆ—è¡¨
COMMON_PLANTS = [
    # èŠ±å‰ç±»
    'ç«ç‘°', 'ç‰¡ä¸¹', 'èŠèŠ±', 'è·èŠ±', 'æ¢…èŠ±', 'å…°èŠ±', 'èŒ‰è‰èŠ±', 'æ¡‚èŠ±',
    'æœˆå­£', 'æœé¹ƒ', 'æ°´ä»™', 'ç™¾åˆ', 'éƒé‡‘é¦™', 'å‘æ—¥è‘µ', 'åº·ä¹ƒé¦¨',
    'èŒ¶èŠ±', 'èŠè¯', 'æµ·æ£ ', 'ç´«ç½—å…°', 'é£ä¿¡å­',
    
    # è§‚å¶æ¤ç‰©
    'ç»¿è', 'åŠå…°', 'è™çš®å…°', 'å‘è´¢æ ‘', 'å¯Œè´µç«¹', 'æ–‡ç«¹', 'é¾ŸèƒŒç«¹',
    'å¸¸æ˜¥è—¤', 'èŠ¦èŸ', 'ä»™äººæŒ', 'å¤šè‚‰æ¤ç‰©', 'é“æ ‘', 'æ©¡çš®æ ‘',
    
    # æ ‘æœ¨ç±»
    'æ¾æ ‘', 'æŸæ ‘', 'æ¨æ ‘', 'æŸ³æ ‘', 'æ§æ ‘', 'æ¢§æ¡', 'é“¶æ', 'æ«æ ‘',
    'æ¨±èŠ±', 'æ¡ƒæ ‘', 'æ¢¨æ ‘', 'è‹¹æœæ ‘', 'ç«¹å­', 'æ£•æ¦ˆ',
    
    # è”¬èœç±»
    'ç•ªèŒ„', 'é»„ç“œ', 'ç™½èœ', 'èåœ', 'èŒ„å­', 'è¾£æ¤’', 'å—ç“œ', 'è¥¿ç“œ',
    
    # è‰æœ¬æ¤ç‰©
    'è–°è¡£è‰', 'è¿·è¿­é¦™', 'è–„è·', 'ç½—å‹’', 'èŠ¦è‹‡', 'ç‹—å°¾è‰',
    
    # è—¤æœ¬æ¤ç‰©
    'è‘¡è„', 'çˆ¬å±±è™', 'ç‰µç‰›èŠ±', 'ç´«è—¤', 'é‡‘é“¶èŠ±',
    
    # æ°´ç”Ÿæ¤ç‰©
    'ç¡è²', 'è²èŠ±', 'æµ®è', 'æ°´è‘«èŠ¦',
    
    # å…¶ä»–å¸¸è§æ¤ç‰©
    'ä»™äººçƒ', 'è’²å…¬è‹±', 'ä¸‰å¶è‰', 'å«ç¾è‰', 'ç‰µç‰›èŠ±', 'é¸¢å°¾',
    'çŸ³æ¦´', 'æ‡æ·', 'æŸ¿å­', 'æ €å­èŠ±', 'å¤¹ç«¹æ¡ƒ', 'å‡¤ä»™èŠ±'
]

if __name__ == '__main__':
    print("ğŸŒ¿ ç™¾åº¦ç™¾ç§‘æ¤ç‰©çˆ¬è™«å¯åŠ¨...")
    print(f"ğŸ“Š è®¡åˆ’çˆ¬å– {len(COMMON_PLANTS)} ç§æ¤ç‰©\n")
    
    crawler = BaikePlantCrawler()
    
    # çˆ¬å–æ•°æ®
    crawler.crawl_plants(COMMON_PLANTS)
    
    # ä¿å­˜æ•°æ®
    crawler.save_to_json('plants_data.json')
    
    print("\nğŸ‰ çˆ¬è™«ä»»åŠ¡å®Œæˆï¼")
